# -*- coding: utf-8 -*-
"""URFU.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nYpg1dScazAsGzGkah1rK882MkzGfIpO
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import zipfile
import seaborn as sns

import requests
from lxml import html
import re
import time
import datetime
from nltk.stem.snowball import SnowballStemmer
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import CountVectorizer

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import TruncatedSVD

from matplotlib import pyplot as plt
import scipy.stats as st
import math
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.isotonic import IsotonicRegression
from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV, LinearRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer, LabelEncoder
from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler 

from sklearn.model_selection import cross_val_score
import seaborn as sns
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import StackingRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.cluster import KMeans


# %matplotlib inline

import warnings
warnings.filterwarnings("ignore")

"""## Получение данных"""

df = pd.read_csv('hakaton.csv')
print(df.shape)
df.head()

df.info()

df.apply(lambda x: x.isna().sum())

df[df['text'].isna()].groupby('social_type').size()

df_new = pd.DataFrame(columns = df.columns)

id_list = df.groupby('id').size().sort_values()

for i, (id, count) in enumerate(zip(id_list.index, id_list.values)):
  print(i)
  if count == 1:
    data = df[df['id'] == id]
    df_new = df_new.append(data)
    continue
  
  if df[df['id'] == id].head(1)['views'].values[0] == -1:
    data = df[df['id'] == id]
    start_date = data.head(1)['created_date']
    finish_date  = data.tail(1)['updated_date']
    new_data = data.tail(1)
    new_data['updated_date'] = pd.to_datetime(finish_date.values) - pd.to_datetime(start_date.values)
    df_new = df_new.append(new_data)
    continue

  data = df[df['id'] == id]
  finish_date  = data.tail(1)['updated_date']
  
  start_views = data.head(1)['views'].values[0]
  finish_views = data.tail(1)['views'].values[0]

  new_data = data.tail(1)
  new_data['views'] =  finish_views - start_views
  new_data['updated_date'] = pd.to_datetime(finish_date.values)
  df_new = df_new.append(new_data)

df_new.shape

df_new.to_csv('datav.02.csv')

df_new.head()

"""## Работа с данными"""

df = pd.read_csv('datav.02.csv', index_col = ['Unnamed: 0'])
df.head()

vk_df = df[df['social_type'] == 'vk']
tg_df = df[df['social_type'] == 'telegram']
od_df = df[df['social_type'] == 'odnoklassniki']
vk_df.shape, tg_df.shape, od_df.shape

"""## работа с vk"""

vk_df.info()

vk_df.head()

"""### Идея №1

Напишем алгоритм, открывающий каждый пост, переходящий на группу/профиль человека и возвращающий охват и дату 5-ти последних постов.
"""

def get_vk_stats(link):
  try:
    group_stats = {}
    group_url = get_group_link(link)
    if group_url in groups.keys():
      print(222)
      return groups[group_url]
    print(group_url)
    if group_url:
      r = requests.Session().get(group_url).content.decode('utf-8')
      tree = html.fromstring(r)
      time.sleep(1)
      if tree.xpath("//h1[@class = 'page_name']//text()") != []:
        print(666)
      group_stats['url'] = group_url
      group_stats['dates'] = tree.xpath("//a[@class = 'wi_date']//text()")
      group_stats['dates'] = [change_date(x) for x in group_stats['dates']]
      group_stats['list_views'] = tree.xpath("//b[@class = 'v_views']//text()")
      group_stats['list_views'] = [value_to_float(x) for x in group_stats['list_views']]
      group_stats['trend'] = [group_stats['dates'][i]/group_stats['list_views'][i]
                              for i in range(len(group_stats['list_views']))]
      group_stats['value'] = pd.Series([group_stats['trend'][i+1]-group_stats['trend'][i]
                              for i in range(len(group_stats['trend']) - 1)]).mean()
      groups[group_url] = group_stats
      return group_stats['value']
    else:
      return 'Not found'
  except:
    return None


def get_group_link(link):
  try:
    r = requests.Session().get(link).content.decode('utf-8')
    tree = html.fromstring(r)
    xpath = tree.xpath("//div[@class = 'wi_head']//@href")
    group_name = re.findall(r'/\w+', xpath[0])
    group_url = 'https://vk.com' + group_name[0]
    return group_url
  except:
    return False

  def value_to_float(x):
    if type(x) == float or type(x) == int:
        return x
    if 'K' in x:
        if len(x) > 1:
            return float(x.replace('K', '')) * 1000
        return 1000.0
    if 'M' in x:
        if len(x) > 1:
            return float(x.replace('M', '')) * 1000000
        return 1000000.0
    if 'B' in x:
        return float(x.replace('B', '')) * 1000000000
    else:
      return float(x)
    return 0.0

  def change_date(x):
    if 'today' in x:
      return 1
    elif 'yesterday' in x:
      return 2
    else:
      date = x.split(' ')
      date_post = datetime.datetime.strptime(f'{date[0]} {date[1]} 2021', '%d %b %Y')
      curr_date = datetime.datetime.today() - date_post
      return curr_date.days

groups = {}

"""К сожалению, данный алгоритм не уклыдвался во временные рамки хакатона"""

vk_df.shape

# values = []
# for id, link in enumerate(vk_df['url'].iloc[:,]):
#   print(id)
#   values.append(get_vk_stats(link))

"""### Идея №2

Используем основные данные

### обработка данных
"""

vk_df['created_date'] = pd.to_datetime(vk_df['created_date'])
vk_df['updated_date'] = pd.to_datetime(vk_df['updated_date'])

"""Находим разность времени размещения поста и последним обновлением"""

vk_df['time'] = pd.to_timedelta(vk_df['updated_date'] - vk_df['created_date']).apply(lambda x: x.total_seconds())

"""Очищаем текст от html"""

vk_df['edit_text'] = vk_df['text'].apply(lambda x: BeautifulSoup(str(x), 'lxml').text)
vk_df['text'] = vk_df['edit_text'].apply(lambda x: x.split('#')[0])
vk_df['hashtags'] =  vk_df['edit_text'].apply(lambda x: ' '.join(re.findall(r'#\w+', x)).strip())

"""Преабразуем текст в число с помощью стемминга -> CountVectorizer"""

stemmer = SnowballStemmer("russian")
vk_df['stem_text'] = vk_df['edit_text'].apply(lambda x: ' '.join([stemmer.stem(w) for w in x.split(' ')]))

vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))
X2 = vectorizer.fit_transform(vk_df['stem_text'])

X3 = vectorizer.fit_transform(vk_df['hashtags'])

X2

X3

"""Видим,что столбцов образовалось довольно много, используем один из методов уменьшения размерности"""

pca = TruncatedSVD(n_components=50, n_iter=7, random_state=42)
new_data = pd.DataFrame(pca.fit_transform(X2), columns = [f'col_{i}' for i in range(1,51)])

pca = TruncatedSVD(n_components=15, n_iter=7, random_state=42)
new_hash = pd.DataFrame(pca.fit_transform(X3), columns = [f'col_hash_{i}' for i in range(1,16)])

vk_df.columns

cols = ['has_images', 'has_videos', 'likes', 'comments', 'reposts', 'is_repost', 'version','author_subscribers','time','views']
for col in cols:
  new_data[col] = vk_df[col].values

for col in new_hash.columns:
  new_data[col] = new_hash[col]

X = new_data.drop(['views'], axis = 1)
y = new_data['views'].values

"""Нормализовываем наши данные для машинного обучения"""

num_scaler = StandardScaler()
cols.remove('views')

for col in cols:
  X[[col]] = num_scaler.fit_transform(X[[col]])

def get_score(target, name, model, X_train, X_test, y_train, y_test, status):
  model.fit(X_train, y_train)
  score_train =model.score(X_train, y_train) 
  score_test = model.score(X_test, y_test)

  if status:
    df_new = pd.DataFrame(model.feature_importances_, index = X_train.columns, columns = ['importance'])
    df_new[df_new['importance']>0].sort_values(by = 'importance').head(15).plot(kind = 'bar', title = f'{name} model and {target} columns')
    plt.show()

  cross_score = cross_val_score(model, X_test, y_test, cv=5)

  return score_train, score_test, cross_score

def get_data(X, y):
  X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.33, random_state=42)

  return  X_train, X_test, y_train, y_test

mls = {
        'GradientBoostingRegressor': GradientBoostingRegressor(random_state=0), 
        'linear': LinearRegression(), 'RandomForestRegressor':RandomForestRegressor(max_depth=2, random_state=0),
        'LassoCV': LassoCV(), 'Ridge': RidgeCV(), 'ElasticNetCV': ElasticNetCV(),
       'ExtraTreesRegressor': ExtraTreesRegressor(n_estimators=100, random_state=0)
        }

"""Тестируем различные модели для окончательного выбора"""

dict_results = {}
status = False

for col in ['views']:
  X_train, X_test, y_train, y_test = get_data(X,y)
  dict_results[col] = {}
  for name, model in mls.items():
    if name not in ['linear', 'LassoCV', 'Ridge', 'ElasticNetCV']:
      status = True
    else:
      status = False

    results = get_score(col, name, model, X_train,  X_test, y_train, y_test, status)
    dict_results[col][name] = {'score_train' : results[0].round(3),'score_test' : results[1].round(3),
                               'cross_score': results[2].mean().round(3)}

for k,v in dict_results.items():
  print(k)
  for ki,vi in v.items():
    print(ki, vi)
  print('-'*30)

"""### Выбираем GradientBoostingRegressor,  LassoCV, ExtraTreesRegressor,RandomForestRegressor

"""

def mean_absolute_percentage_error(y_true, y_pred): 
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

"""### GradientBoostingRegressor"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle = True)


parameters = {'learning_rate':  [0.1],           #[0.1, 0.3, 0.9],
                      'max_depth':  [90],             #[80, 90],
                      'max_features':  [3],             #[2, 3],
                      'min_samples_leaf': [4] }      #[3, 4, 5]}



model1 = GradientBoostingRegressor(random_state= 42)
grid = GridSearchCV(model1, parameters,  cv=3, verbose=True,
                               n_jobs= -1, scoring = 'r2')
grid.fit(X_train, y_train) 
grid.best_estimator_

grad_boost = pd.DataFrame({
    'r2_train': [r2_score(y_train, grid.predict(X_train)).round(6)],
    'r2_test': [r2_score(y_test, grid.predict(X_test)).round(6)],
    'MAE': [mean_absolute_error(y_test, grid.predict(X_test)).round(6)],
    'MSE': [mean_squared_error(y_test, grid.predict(X_test)).round(6)],
    'MAPE': [mean_absolute_percentage_error(y_test, grid.predict(X_test)).round(6)]}, index = ['GradientBoostingRegressor'])
grad_boost

"""### LassoCV"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle = True)


parameters = {'n_alphas':  [120],                     #[80, 90,100,110,120]}
              'max_iter': [900, 950, 1000]}           # 900


model1 = LassoCV(random_state = 42)
grid2 = GridSearchCV(model1, parameters,  cv=5, verbose=True,
                               n_jobs= -1)
grid2.fit(X_train, y_train) 
grid2.best_estimator_

grad_lasso = pd.DataFrame({
    'r2_train': [r2_score(y_train, grid2.predict(X_train)).round(6)],
    'r2_test': [r2_score(y_test, grid2.predict(X_test)).round(6)],
    'MAE': [mean_absolute_error(y_test, grid2.predict(X_test)).round(6)],
    'MSE': [mean_squared_error(y_test, grid2.predict(X_test)).round(3)],
    'MAPE': [mean_absolute_percentage_error(y_test, grid2.predict(X_test)).round(6)]}, index = ['lasso'])
grad_lasso

"""### RandomForestRegressor"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle = True)


parameters = {'n_estimators': [150],         #[110,120, 150]
              'max_depth': [15]}        #[15, 25, 40]


model1 = RandomForestRegressor(random_state = 42)
grid3 = GridSearchCV(model1, parameters,  cv=5, verbose=True,
                               n_jobs= -1)
grid3.fit(X_train, y_train) 
grid3.best_estimator_

grad_forest = pd.DataFrame({
    'r2_train': [r2_score(y_train, grid3.predict(X_train)).round(6)],
    'r2_test': [r2_score(y_test, grid3.predict(X_test)).round(6)],
    'MAE': [mean_absolute_error(y_test, grid3.predict(X_test)).round(6)],
    'MSE': [mean_squared_error(y_test, grid3.predict(X_test)).round(3)],
    'MAPE': [mean_absolute_percentage_error(y_test, grid3.predict(X_test)).round(6)]}, index = ['RandomForest'])
grad_forest

"""### ExtraTreesRegressor"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle = True)


parameters = {'n_estimators':  [150],           #[110,120, 150, 200],
              'max_depth': [7] }           #[20,30]} 


model1 = ExtraTreesRegressor(random_state = 42)
grid4 = GridSearchCV(model1, parameters,  cv=5, verbose=True,
                               n_jobs= -1)
grid4.fit(X_train, y_train) 
grid4.best_estimator_

grad_extra = pd.DataFrame({
    'r2_train': [r2_score(y_train, grid4.predict(X_train)).round(6)],
    'r2_test': [r2_score(y_test, grid4.predict(X_test)).round(6)],
    'MAE': [mean_absolute_error(y_test, grid4.predict(X_test)).round(6)],
    'MSE': [mean_squared_error(y_test, grid4.predict(X_test)).round(3)],
    'MAPE': [mean_absolute_percentage_error(y_test, grid4.predict(X_test)).round(6)]}, index = ['ExtraTrees'])
grad_extra

"""Используем стеккинг моделей для получения лучшего результата наших обученных моделей

### Stacking
"""

estimators = [
    ('GradientBoosting', grid.best_estimator_),
    ('LassoCV', grid2.best_estimator_),
    ('ExtraTrees', grid4.best_estimator_),
    ('RandomForest', grid3.best_estimator_)
]
reg = StackingRegressor(
    estimators=estimators)
reg.fit(X_train, y_train)

new1 = pd.DataFrame({
    'r2_train': [r2_score(y_train, reg.predict(X_train)).round(6)],
    'r2_test': [r2_score(y_test, reg.predict(X_test)).round(6)],
    'MAE': [mean_absolute_error(y_test, reg.predict(X_test)).round(6)],
    'MSE': [mean_squared_error(y_test, reg.predict(X_test)).round(3)],
    'MAPE': [mean_absolute_percentage_error(y_test, reg.predict(X_test)).round(6)]}, index = ['Stacking'])
new1

all_vk = pd.concat([grad_boost, grad_lasso, grad_extra, new1])
all_vk

views = pd.DataFrame()
for id_post in range(5):
  view = []
  for day in range(1,11):
    X = new_data.drop(['views'], axis = 1)
    X.loc[id_post,'time'] += 60*60*24*day
    for col in cols:
      X[[col]] = num_scaler.fit_transform(X[[col]])
    view.append(reg.predict([X.loc[id_post,:]])[0] + y[id_post])
  views[f"{df.loc[:,['author_id']].values[id_post][0]} id"] = view

views.plot(figsize = (10,4))

"""## работа с telegram

Делаем все тоже самое, только с данными из телеграмма

### обработка данных
"""

tg_df['created_date'] = pd.to_datetime(tg_df['created_date'])
tg_df['updated_date'] = pd.to_datetime(tg_df['updated_date'])

tg_df['time'] = pd.to_timedelta(tg_df['updated_date'] - tg_df['created_date']).apply(lambda x: x.total_seconds())

tg_df['edit_text'] = tg_df['text'].apply(lambda x: BeautifulSoup(str(x), 'lxml').text)
tg_df['text'] = tg_df['edit_text'].apply(lambda x: x.split('#')[0])
tg_df['hashtags'] =  tg_df['edit_text'].apply(lambda x: ' '.join(re.findall(r'#\w+', x)).strip())

stemmer = SnowballStemmer("russian")
tg_df['stem_text'] = tg_df['edit_text'].apply(lambda x: ' '.join([stemmer.stem(w) for w in x.split(' ')]))

vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))
X2 = vectorizer.fit_transform(tg_df['stem_text'])

X3 = vectorizer.fit_transform(tg_df['hashtags'])

pca = TruncatedSVD(n_components=50, n_iter=7, random_state=42)
new_data = pd.DataFrame(pca.fit_transform(X2), columns = [f'col_{i}' for i in range(1,51)])

pca = TruncatedSVD(n_components=15, n_iter=7, random_state=42)
new_hash = pd.DataFrame(pca.fit_transform(X3), columns = [f'col_hash_{i}' for i in range(1,16)])

cols = ['has_images', 'has_videos', 'version','author_subscribers','time','views']
for col in cols:
  new_data[col] = tg_df[col].values

for col in new_hash.columns:
  new_data[col] = new_hash[col]

X = new_data.drop(['views'], axis = 1)
y = new_data['views'].values

num_scaler = StandardScaler()
cols.remove('views')

for col in cols:
  X[[col]] = num_scaler.fit_transform(X[[col]])

def mean_absolute_percentage_error(y_true, y_pred): 
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

dict_results = {}
status = False

for col in ['views']:
  X_train, X_test, y_train, y_test = get_data(X,y)
  dict_results[col] = {}
  for name, model in mls.items():
    if name not in ['linear', 'LassoCV', 'Ridge', 'ElasticNetCV']:
      status = True
    else:
      status = False

    results = get_score(col, name, model, X_train,  X_test, y_train, y_test, status)
    dict_results[col][name] = {'score_train' : results[0].round(3),'score_test' : results[1].round(3),
                               'cross_score': results[2].mean().round(3)}

for k,v in dict_results.items():
  print(k)
  for ki,vi in v.items():
    print(ki, vi)
  print('-'*30)

"""### Выбираем GradientBoostingRegressor, RandomForestRegressor, LassoCV

### GradientBoostingRegressor
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle = True)


parameters = {'learning_rate':  [0.1],           #[0.1,0.3,0.5,0.7,0.9],
                      'max_depth':  [20],             #[20,30,40,50],
                      'max_features':  [2],             #[2, 3],
                      'min_samples_leaf': [5] }      #[3, 4, 5]}



model1 = GradientBoostingRegressor(random_state= 42)
grid = GridSearchCV(model1, parameters,  cv=3, verbose=True,
                               n_jobs= -1, scoring = 'r2')
grid.fit(X_train, y_train) 
grid.best_estimator_

grad_boost = pd.DataFrame({
    'r2_train': [r2_score(y_train, grid.predict(X_train)).round(6)],
    'r2_test': [r2_score(y_test, grid.predict(X_test)).round(6)],
    'MAE': [mean_absolute_error(y_test, grid.predict(X_test)).round(6)],
    'MSE': [mean_squared_error(y_test, grid.predict(X_test)).round(6)],
    'MAPE': [mean_absolute_percentage_error(y_test, grid.predict(X_test)).round(6)]}, index = ['GradientBoostingRegressor'])
grad_boost

"""### RandomForestRegressor"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle = True)


parameters = {'n_estimators': [200],         #[110,120, 150]
              'max_depth': [15]}        #[15, 25, 40]


model1 = RandomForestRegressor(random_state = 42)
grid3 = GridSearchCV(model1, parameters,  cv=5, verbose=True,
                               n_jobs= -1)
grid3.fit(X_train, y_train) 
grid3.best_estimator_

grad_forest = pd.DataFrame({
    'r2_train': [r2_score(y_train, grid3.predict(X_train)).round(6)],
    'r2_test': [r2_score(y_test, grid3.predict(X_test)).round(6)],
    'MAE': [mean_absolute_error(y_test, grid3.predict(X_test)).round(6)],
    'MSE': [mean_squared_error(y_test, grid3.predict(X_test)).round(3)],
    'MAPE': [mean_absolute_percentage_error(y_test, grid3.predict(X_test)).round(6)]}, index = ['RandomForest'])
grad_forest

"""### LassoCV"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle = True)


parameters = {'n_alphas':  [120,130,140],                     #[80, 90,100,110,120]}
              'max_iter': [900, 950, 1000]}           # 900


model1 = LassoCV(random_state = 42)
grid2 = GridSearchCV(model1, parameters,  cv=5, verbose=True,
                               n_jobs= -1)
grid2.fit(X_train, y_train) 
grid2.best_estimator_

grad_lasso = pd.DataFrame({
    'r2_train': [r2_score(y_train, grid2.predict(X_train)).round(6)],
    'r2_test': [r2_score(y_test, grid2.predict(X_test)).round(6)],
    'MAE': [mean_absolute_error(y_test, grid2.predict(X_test)).round(6)],
    'MSE': [mean_squared_error(y_test, grid2.predict(X_test)).round(3)],
    'MAPE': [mean_absolute_percentage_error(y_test, grid2.predict(X_test)).round(6)]}, index = ['lasso'])
grad_lasso

"""### Stacking"""

estimators = [
    ('GradientBoosting', grid.best_estimator_),
    ('LassoCV', grid2.best_estimator_),
    ('RandomForest', grid3.best_estimator_)
]
reg_tg = StackingRegressor(
    estimators=estimators)
reg_tg.fit(X_train, y_train)

new1 = pd.DataFrame({
    'r2_train': [r2_score(y_train, reg_tg.predict(X_train)).round(6)],
    'r2_test': [r2_score(y_test, reg_tg.predict(X_test)).round(6)],
    'MAE': [mean_absolute_error(y_test, reg_tg.predict(X_test)).round(6)],
    'MSE': [mean_squared_error(y_test, reg_tg.predict(X_test)).round(6)],
    'MAPE': [mean_absolute_percentage_error(y_test, reg_tg.predict(X_test)).round(6)]}, index = ['Stacking'])
new1

all_tg = pd.concat([grad_boost, grad_lasso, grad_forest, new1])
all_tg

pred_df=pd.DataFrame({'true':y_test,'pred':reg_tg.predict(X_test)})
plt.figure(figsize=(15,5))
plt.scatter(range(pred_df.shape[0]),pred_df['true'], label = 'y_true', alpha = 0.6)
plt.scatter(range(pred_df.shape[0]),pred_df['pred'], label = 'y_pred', alpha = 0.9)
plt.ylim(0, 200)
plt.legend()
plt.show()

X = new_data.drop(['views'], axis = 1)
y = new_data['views'].values

views = pd.DataFrame()
for id_post in range(5):
  view = []
  for day in range(1,11):
    X = new_data.drop(['views'], axis = 1)
    X.loc[id_post,'time'] += 60*60*24*day
    for col in cols:
      X[[col]] = num_scaler.fit_transform(X[[col]])
    view.append(reg_tg.predict([X.loc[id_post,:]])[0] + y[id_post])
  views[f"{tg_df.loc[:,['author_id']].values[id_post][0]} id"] = view

views.plot(figsize = (10,4))

"""## работа с одноклассниками"""

df = pd.read_csv('datav.02.csv')
od_df = df[df['social_type'] == 'odnoklassniki']

od_df.info()

def get_time(i):
  try:
    o = od_df['updated_date'].iloc[i,]
    o = datetime.datetime.strptime(o, '%Y-%m-%d %H:%M:%S')
    k = (o - pd.to_datetime(od_df['created_date'].iloc[i,])).total_seconds()
    return k
  except:
    k = pd.Timedelta('12 days 23:14:26').total_seconds()
    return k

od_df['time'] = [abs(get_time(i)) for i in range(od_df.shape[0])]

od_df['edit_text'] = od_df['text'].apply(lambda x: BeautifulSoup(str(x), 'lxml').text)
od_df['text'] = od_df['edit_text'].apply(lambda x: x.split('#')[0])
od_df['hashtags'] =  od_df['edit_text'].apply(lambda x: ' '.join(re.findall(r'#\w+', x)).strip())

stemmer = SnowballStemmer("russian")
od_df['stem_text'] = od_df['edit_text'].apply(lambda x: ' '.join([stemmer.stem(w) for w in x.split(' ')]))

vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))
X2 = vectorizer.fit_transform(od_df['stem_text'])

X3 = vectorizer.fit_transform(od_df['hashtags'])

pca = TruncatedSVD(n_components=5, n_iter=7, random_state=42)
new_data = pd.DataFrame(pca.fit_transform(X2), columns = [f'col_{i}' for i in range(1,6)])

pca = TruncatedSVD(n_components=3, n_iter=7, random_state=42)
new_hash = pd.DataFrame(pca.fit_transform(X3), columns = [f'col_hash_{i}' for i in range(1,4)])

od_df.head(1)

cols = ['has_images', 'has_videos', 'likes', 'comments', 'version', 'reposts', 'is_repost', 'version', 'author_subscribers','time']
for col in cols:
  new_data[col] = od_df[col].values

for col in new_hash.columns:
  new_data[col] = new_hash[col]

for col in cols:
  new_data[[col]] = num_scaler.fit_transform(new_data[[col]])

new_data

link = linkage(new_data, 'ward', 'euclidean')

dist = link[-50:, 2]
dist_rev = dist[::-1]
idxs = range(1, len(dist) + 1)

fig, ax = plt.subplots()
fig.set_size_inches(11.7, 8.27)
plt.plot(idxs, dist_rev, marker='o')
plt.title('Distance between merged clusters')
plt.xlabel('Step')
plt.ylabel('Distance')

plt.show()

od_df['cluster'] = fcluster(link, 8, criterion='maxclust')

info = od_df.groupby('cluster').mean()[cols]
info['time'] = info['time']/60/60
info





info['k1'] = 100*(info['likes'] + info['comments'])/info['author_subscribers']
info['k2'] = (info['likes'])/info['time']

info.sort_values(by='k1')

"""* cluster 2 - хорошая, живая группа, охват средний,  подписчиков много, однако активных мало, преимущественно с фото
* cluster 4 - хорошая живая группа, охват средний, подписчиков мало для такого с фото и без  
* cluster 1 - группа с накруткой подписчиков, кол-во лайков среднее по отношению к общему количеству
* cluster 7 - развивающееся группа, кол-во активных подписчиков растет, хороший потенциал для дальнейшего сотрудничества
* cluster 8 - развивающееся группа, кол-во активных подписчиков растет, в будущем, возможно, будет хороший потенциал для сотрудничества
* cluster 5 - мертвая группа, возможно сильная накрутка или перепродажа сообщества
* cluster 6 - средний сегмент группы, количество подписчиков среднее, интересный вариант для сотрудничества
* cluster 3 - группа полумертвая, подписчики неохотно просматривают посты.
"""

K = range(1, 15)
models = [KMeans(n_clusters=k, random_state=42).fit(new_data) for k in K]
dist = [model.inertia_ for model in models]
#Метод inertia_ вернёт сумму расстояний от каждой точки данных до центра ближайшего у ней кластера. 
#кластеризацию можно считать условно хорошей, когда инерция перестаёт сильно уменьшаться при увеличении числа кластеров.



fig, ax = plt.subplots()
fig.set_size_inches(11.7, 8.27)
plt.plot(K, dist, marker='o')
plt.xlabel('k')
plt.ylabel('Sum of distances')
plt.title('The Elbow Method showing the optimal k')
plt.show()